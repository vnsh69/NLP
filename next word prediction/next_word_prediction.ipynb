{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GRU, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the data\n",
    "with open(\"1661-0.txt\", \"r\") as file:\n",
    "    text = file.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8923"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokenizing the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating training sample where each sequence gets progressively increase\n",
    "sequences = []\n",
    "for line in text.split('.'):  ## spliting new sentences\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]  ## converting word(sentence) to tokenized list\n",
    "    for i in range (1,len(token_list)):  ## start from 1 bcs it ensure that list atleast have 2 words(token)\n",
    "        n_gram_sequence = token_list[:i+1]    \n",
    "        sequences.append(n_gram_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "max_sequence_len = max(len(x) for x in sequences)\n",
    "print(max_sequence_len)\n",
    "\n",
    "## padding to sequence so that input size is fixed\n",
    "input_sequence = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = input_sequence[:,:-1], input_sequence[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(y, num_classes = total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor='accuracy',  restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating model using LSTM\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=total_words, output_dim=100, input_length=max_sequence_len - 1),\n",
    "    LSTM(100),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(total_words, activation='softmax')  ## Predict next word\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 104, 100)          892300    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 100)              400       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8923)              901223    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,884,423\n",
      "Trainable params: 1,884,223\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "2616/2616 [==============================] - 43s 14ms/step - loss: 6.3115 - accuracy: 0.0913\n",
      "Epoch 2/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 5.5059 - accuracy: 0.1349\n",
      "Epoch 3/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 5.1029 - accuracy: 0.1550\n",
      "Epoch 4/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 4.7944 - accuracy: 0.1737\n",
      "Epoch 5/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 4.5355 - accuracy: 0.1892\n",
      "Epoch 6/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 4.3141 - accuracy: 0.2038\n",
      "Epoch 7/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 4.1275 - accuracy: 0.2180\n",
      "Epoch 8/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 3.9550 - accuracy: 0.2303\n",
      "Epoch 9/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 3.8135 - accuracy: 0.2434\n",
      "Epoch 10/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 3.6810 - accuracy: 0.2542\n",
      "Epoch 11/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 3.5662 - accuracy: 0.2667\n",
      "Epoch 12/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 3.4583 - accuracy: 0.2778\n",
      "Epoch 13/120\n",
      "2616/2616 [==============================] - 55s 21ms/step - loss: 3.3677 - accuracy: 0.2901\n",
      "Epoch 14/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 3.2821 - accuracy: 0.3033\n",
      "Epoch 15/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 3.2059 - accuracy: 0.3119\n",
      "Epoch 16/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 3.1323 - accuracy: 0.3242\n",
      "Epoch 17/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 3.0635 - accuracy: 0.3333\n",
      "Epoch 18/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 3.0056 - accuracy: 0.3411\n",
      "Epoch 19/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 2.9454 - accuracy: 0.3513\n",
      "Epoch 20/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 2.8950 - accuracy: 0.3569\n",
      "Epoch 21/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 2.8421 - accuracy: 0.3663\n",
      "Epoch 22/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 2.8028 - accuracy: 0.3723\n",
      "Epoch 23/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 2.7454 - accuracy: 0.3822\n",
      "Epoch 24/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 2.7113 - accuracy: 0.3875\n",
      "Epoch 25/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 2.6697 - accuracy: 0.3937\n",
      "Epoch 26/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 2.6377 - accuracy: 0.3994\n",
      "Epoch 27/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 2.5959 - accuracy: 0.4066\n",
      "Epoch 28/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 2.5650 - accuracy: 0.4114\n",
      "Epoch 29/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 2.5319 - accuracy: 0.4176\n",
      "Epoch 30/120\n",
      "2616/2616 [==============================] - 36s 14ms/step - loss: 2.5074 - accuracy: 0.4206\n",
      "Epoch 31/120\n",
      "2616/2616 [==============================] - 39s 15ms/step - loss: 2.4602 - accuracy: 0.4284\n",
      "Epoch 32/120\n",
      "2616/2616 [==============================] - 41s 16ms/step - loss: 2.4341 - accuracy: 0.4333\n",
      "Epoch 33/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.4184 - accuracy: 0.4368\n",
      "Epoch 34/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.3863 - accuracy: 0.4431\n",
      "Epoch 35/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.3551 - accuracy: 0.4478\n",
      "Epoch 36/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.3319 - accuracy: 0.4524\n",
      "Epoch 37/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.3109 - accuracy: 0.4565\n",
      "Epoch 38/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.2925 - accuracy: 0.4598\n",
      "Epoch 39/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.2744 - accuracy: 0.4634\n",
      "Epoch 40/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.2442 - accuracy: 0.4709\n",
      "Epoch 41/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.2213 - accuracy: 0.4700\n",
      "Epoch 42/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.2052 - accuracy: 0.4763\n",
      "Epoch 43/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.1887 - accuracy: 0.4775\n",
      "Epoch 44/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.1723 - accuracy: 0.4831\n",
      "Epoch 45/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.1462 - accuracy: 0.4881\n",
      "Epoch 46/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.1370 - accuracy: 0.4885\n",
      "Epoch 47/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.1114 - accuracy: 0.4925\n",
      "Epoch 48/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.0926 - accuracy: 0.4955\n",
      "Epoch 49/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.0878 - accuracy: 0.4990\n",
      "Epoch 50/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.0653 - accuracy: 0.5021\n",
      "Epoch 51/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.0497 - accuracy: 0.5062\n",
      "Epoch 52/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.0309 - accuracy: 0.5092\n",
      "Epoch 53/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.0158 - accuracy: 0.5119\n",
      "Epoch 54/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 2.0117 - accuracy: 0.5133\n",
      "Epoch 55/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.9940 - accuracy: 0.5165\n",
      "Epoch 56/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.9782 - accuracy: 0.5206\n",
      "Epoch 57/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.9730 - accuracy: 0.5190\n",
      "Epoch 58/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.9603 - accuracy: 0.5223\n",
      "Epoch 59/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.9485 - accuracy: 0.5256\n",
      "Epoch 60/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.9312 - accuracy: 0.5280\n",
      "Epoch 61/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.9181 - accuracy: 0.5314\n",
      "Epoch 62/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.9179 - accuracy: 0.5315\n",
      "Epoch 63/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.9055 - accuracy: 0.5344\n",
      "Epoch 64/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.8897 - accuracy: 0.5374\n",
      "Epoch 65/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.8793 - accuracy: 0.5391\n",
      "Epoch 66/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.8721 - accuracy: 0.5417\n",
      "Epoch 67/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.8619 - accuracy: 0.5429\n",
      "Epoch 68/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.8474 - accuracy: 0.5458\n",
      "Epoch 69/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.8386 - accuracy: 0.5465\n",
      "Epoch 70/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.8221 - accuracy: 0.5513\n",
      "Epoch 71/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.8192 - accuracy: 0.5526\n",
      "Epoch 72/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.8071 - accuracy: 0.5546\n",
      "Epoch 73/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.8058 - accuracy: 0.5538\n",
      "Epoch 74/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.7864 - accuracy: 0.5586\n",
      "Epoch 75/120\n",
      "2616/2616 [==============================] - 42s 16ms/step - loss: 1.7809 - accuracy: 0.5584\n",
      "Epoch 76/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.7773 - accuracy: 0.5598\n",
      "Epoch 77/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.7669 - accuracy: 0.5613\n",
      "Epoch 78/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.7601 - accuracy: 0.5635\n",
      "Epoch 79/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.7402 - accuracy: 0.5663\n",
      "Epoch 80/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.7519 - accuracy: 0.5655\n",
      "Epoch 81/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.7374 - accuracy: 0.5676\n",
      "Epoch 82/120\n",
      "2616/2616 [==============================] - 29s 11ms/step - loss: 1.7242 - accuracy: 0.5719\n",
      "Epoch 83/120\n",
      "2616/2616 [==============================] - 23s 9ms/step - loss: 1.7257 - accuracy: 0.5723\n",
      "Epoch 84/120\n",
      "2616/2616 [==============================] - 59s 22ms/step - loss: 1.7092 - accuracy: 0.5735\n",
      "Epoch 85/120\n",
      "2616/2616 [==============================] - 65s 25ms/step - loss: 1.7051 - accuracy: 0.5742\n",
      "Epoch 86/120\n",
      "2616/2616 [==============================] - 44s 17ms/step - loss: 1.7024 - accuracy: 0.5762\n",
      "Epoch 87/120\n",
      "2616/2616 [==============================] - 25s 9ms/step - loss: 1.6929 - accuracy: 0.5763\n",
      "Epoch 88/120\n",
      "2616/2616 [==============================] - 25s 9ms/step - loss: 1.6857 - accuracy: 0.5788\n",
      "Epoch 89/120\n",
      "2616/2616 [==============================] - 25s 10ms/step - loss: 1.6755 - accuracy: 0.5819\n",
      "Epoch 90/120\n",
      "2616/2616 [==============================] - 25s 9ms/step - loss: 1.6795 - accuracy: 0.5806\n",
      "Epoch 91/120\n",
      "2616/2616 [==============================] - 25s 9ms/step - loss: 1.6590 - accuracy: 0.5843\n",
      "Epoch 92/120\n",
      "2616/2616 [==============================] - 25s 9ms/step - loss: 1.6631 - accuracy: 0.5857\n",
      "Epoch 93/120\n",
      "2616/2616 [==============================] - 25s 10ms/step - loss: 1.6520 - accuracy: 0.5867\n",
      "Epoch 94/120\n",
      "2616/2616 [==============================] - 26s 10ms/step - loss: 1.6495 - accuracy: 0.5885\n",
      "Epoch 95/120\n",
      "2616/2616 [==============================] - 26s 10ms/step - loss: 1.6415 - accuracy: 0.5900\n",
      "Epoch 96/120\n",
      "2616/2616 [==============================] - 24s 9ms/step - loss: 1.6383 - accuracy: 0.5899\n",
      "Epoch 97/120\n",
      "2616/2616 [==============================] - 24s 9ms/step - loss: 1.6297 - accuracy: 0.5903\n",
      "Epoch 98/120\n",
      "2616/2616 [==============================] - 24s 9ms/step - loss: 1.6169 - accuracy: 0.5947\n",
      "Epoch 99/120\n",
      "2616/2616 [==============================] - 24s 9ms/step - loss: 1.6092 - accuracy: 0.5963\n",
      "Epoch 100/120\n",
      "2616/2616 [==============================] - 24s 9ms/step - loss: 1.6085 - accuracy: 0.5952\n",
      "Epoch 101/120\n",
      "2616/2616 [==============================] - 22s 9ms/step - loss: 1.6091 - accuracy: 0.5958\n",
      "Epoch 102/120\n",
      "2616/2616 [==============================] - 22s 8ms/step - loss: 1.6020 - accuracy: 0.6003\n",
      "Epoch 103/120\n",
      "2616/2616 [==============================] - 23s 9ms/step - loss: 1.5951 - accuracy: 0.5992\n",
      "Epoch 104/120\n",
      "2616/2616 [==============================] - 23s 9ms/step - loss: 1.5917 - accuracy: 0.6019\n",
      "Epoch 105/120\n",
      "2616/2616 [==============================] - 37s 14ms/step - loss: 1.5769 - accuracy: 0.6042\n",
      "Epoch 106/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 1.5718 - accuracy: 0.6047\n",
      "Epoch 107/120\n",
      "2616/2616 [==============================] - 65s 25ms/step - loss: 1.5766 - accuracy: 0.6036\n",
      "Epoch 108/120\n",
      "2616/2616 [==============================] - 48s 18ms/step - loss: 1.5780 - accuracy: 0.6032\n",
      "Epoch 109/120\n",
      "2616/2616 [==============================] - 23s 9ms/step - loss: 1.5639 - accuracy: 0.6062\n",
      "Epoch 110/120\n",
      "2616/2616 [==============================] - 22s 9ms/step - loss: 1.5587 - accuracy: 0.6072\n",
      "Epoch 111/120\n",
      "2616/2616 [==============================] - 22s 9ms/step - loss: 1.5586 - accuracy: 0.6067\n",
      "Epoch 112/120\n",
      "2616/2616 [==============================] - 22s 8ms/step - loss: 1.5448 - accuracy: 0.6110\n",
      "Epoch 113/120\n",
      "2616/2616 [==============================] - 22s 8ms/step - loss: 1.5462 - accuracy: 0.6112\n",
      "Epoch 114/120\n",
      "2616/2616 [==============================] - 22s 8ms/step - loss: 1.5469 - accuracy: 0.6107\n",
      "Epoch 115/120\n",
      "2616/2616 [==============================] - 22s 9ms/step - loss: 1.5316 - accuracy: 0.6117\n",
      "Epoch 116/120\n",
      "2616/2616 [==============================] - 23s 9ms/step - loss: 1.5371 - accuracy: 0.6119\n",
      "Epoch 117/120\n",
      "2616/2616 [==============================] - 23s 9ms/step - loss: 1.5221 - accuracy: 0.6161\n",
      "Epoch 118/120\n",
      "2616/2616 [==============================] - 23s 9ms/step - loss: 1.5183 - accuracy: 0.6162\n",
      "Epoch 119/120\n",
      "2616/2616 [==============================] - 22s 8ms/step - loss: 1.5097 - accuracy: 0.6193\n",
      "Epoch 120/120\n",
      "2616/2616 [==============================] - 23s 9ms/step - loss: 1.5149 - accuracy: 0.6178\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, input_text, tokenizer, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    if len(token_list)>=max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]\n",
    "    \n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list)\n",
    "    predicted_max_index = np.argmax(predicted,axis=1)\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_max_index:\n",
    "            return word\n",
    "        \n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 472ms/step\n",
      "The note was undated, and without either signature or ,prediction: address\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The note was undated, and without either signature or\"\n",
    "word = predict_next_word(model=model, input_text=input_text, tokenizer=tokenizer, max_sequence_len=max_sequence_len)\n",
    "print(f\"{input_text} ,prediction: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "Peculiar—that is the very word,” said ,prediction: holmes\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Peculiar—that is the very word,” said\"\n",
    "word = predict_next_word(model=model, input_text=input_text, tokenizer=tokenizer, max_sequence_len=max_sequence_len)\n",
    "print(f\"{input_text} ,prediction: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "“Very, indeed. And what does she propose to do with the ,prediction: photograph\n"
     ]
    }
   ],
   "source": [
    "input_text = \"“Very, indeed. And what does she propose to do with the\"\n",
    "word = predict_next_word(model=model, input_text=input_text, tokenizer=tokenizer, max_sequence_len=max_sequence_len)\n",
    "print(f\"{input_text} ,prediction: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model.save(\"next_word_predition.h5\")\n",
    "with open('tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using Bidirectional with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating model using bidirectional GRU\n",
    "model_bidirectonal = Sequential([\n",
    "    Embedding(input_dim=total_words, output_dim=50, input_length=max_sequence_len - 1),\n",
    "    Bidirectional(GRU(128, return_sequences=True)),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    Bidirectional(GRU(64)),\n",
    "    Dense(total_words, activation='softmax')  ## Predict next word\n",
    "])\n",
    "\n",
    "model_bidirectonal.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 104, 50)           446150    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 104, 256)         138240    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 104, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 104, 256)         1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              123648    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8923)              1151067   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,860,129\n",
      "Trainable params: 1,859,617\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bidirectonal.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "2616/2616 [==============================] - 68s 23ms/step - loss: 6.4356 - accuracy: 0.0760\n",
      "Epoch 2/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 5.6118 - accuracy: 0.1283\n",
      "Epoch 3/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 5.2093 - accuracy: 0.1517\n",
      "Epoch 4/120\n",
      "2616/2616 [==============================] - 69s 26ms/step - loss: 4.8924 - accuracy: 0.1701\n",
      "Epoch 5/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 4.6107 - accuracy: 0.1866\n",
      "Epoch 6/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 4.3562 - accuracy: 0.2021\n",
      "Epoch 7/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 4.1278 - accuracy: 0.2219\n",
      "Epoch 8/120\n",
      "2616/2616 [==============================] - 63s 24ms/step - loss: 3.9361 - accuracy: 0.2373\n",
      "Epoch 9/120\n",
      "2616/2616 [==============================] - 65s 25ms/step - loss: 3.7347 - accuracy: 0.2580\n",
      "Epoch 10/120\n",
      "2616/2616 [==============================] - 65s 25ms/step - loss: 3.5582 - accuracy: 0.2797\n",
      "Epoch 11/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 3.4014 - accuracy: 0.3016\n",
      "Epoch 12/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 3.2494 - accuracy: 0.3225\n",
      "Epoch 13/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 3.1118 - accuracy: 0.3429\n",
      "Epoch 14/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 2.9882 - accuracy: 0.3634\n",
      "Epoch 15/120\n",
      "2616/2616 [==============================] - 67s 25ms/step - loss: 2.8750 - accuracy: 0.3805\n",
      "Epoch 16/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 2.7675 - accuracy: 0.3992\n",
      "Epoch 17/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 2.6738 - accuracy: 0.4136\n",
      "Epoch 18/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 2.5823 - accuracy: 0.4293\n",
      "Epoch 19/120\n",
      "2616/2616 [==============================] - 67s 25ms/step - loss: 2.5001 - accuracy: 0.4452\n",
      "Epoch 20/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 2.4239 - accuracy: 0.4581\n",
      "Epoch 21/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 2.3488 - accuracy: 0.4705\n",
      "Epoch 22/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 2.2894 - accuracy: 0.4821\n",
      "Epoch 23/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 2.2226 - accuracy: 0.4939\n",
      "Epoch 24/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 2.1617 - accuracy: 0.5044\n",
      "Epoch 25/120\n",
      "2616/2616 [==============================] - 67s 25ms/step - loss: 2.1081 - accuracy: 0.5158\n",
      "Epoch 26/120\n",
      "2616/2616 [==============================] - 67s 25ms/step - loss: 2.0499 - accuracy: 0.5266\n",
      "Epoch 27/120\n",
      "2616/2616 [==============================] - 67s 25ms/step - loss: 2.0048 - accuracy: 0.5347\n",
      "Epoch 28/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 1.9574 - accuracy: 0.5446\n",
      "Epoch 29/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 1.9111 - accuracy: 0.5536\n",
      "Epoch 30/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 1.8673 - accuracy: 0.5614\n",
      "Epoch 31/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 1.8259 - accuracy: 0.5697\n",
      "Epoch 32/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 1.7889 - accuracy: 0.5777\n",
      "Epoch 33/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 1.7455 - accuracy: 0.5850\n",
      "Epoch 34/120\n",
      "2616/2616 [==============================] - 68s 26ms/step - loss: 1.7131 - accuracy: 0.5928\n",
      "Epoch 35/120\n",
      "2616/2616 [==============================] - 68s 26ms/step - loss: 1.6779 - accuracy: 0.5997\n",
      "Epoch 36/120\n",
      "2616/2616 [==============================] - 68s 26ms/step - loss: 1.6448 - accuracy: 0.6051\n",
      "Epoch 37/120\n",
      "2616/2616 [==============================] - 68s 26ms/step - loss: 1.6126 - accuracy: 0.6139\n",
      "Epoch 38/120\n",
      "2616/2616 [==============================] - 67s 25ms/step - loss: 1.6120 - accuracy: 0.6140\n",
      "Epoch 39/120\n",
      "2616/2616 [==============================] - 61s 23ms/step - loss: 1.7084 - accuracy: 0.5857\n",
      "Epoch 40/120\n",
      "2616/2616 [==============================] - 57s 22ms/step - loss: 1.5474 - accuracy: 0.6255\n",
      "Epoch 41/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.4905 - accuracy: 0.6390\n",
      "Epoch 42/120\n",
      "2616/2616 [==============================] - 56s 21ms/step - loss: 1.4650 - accuracy: 0.6443\n",
      "Epoch 43/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.4482 - accuracy: 0.6470\n",
      "Epoch 44/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.4364 - accuracy: 0.6495\n",
      "Epoch 45/120\n",
      "2616/2616 [==============================] - 57s 22ms/step - loss: 1.4150 - accuracy: 0.6519\n",
      "Epoch 46/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.3905 - accuracy: 0.6594\n",
      "Epoch 47/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.3679 - accuracy: 0.6635\n",
      "Epoch 48/120\n",
      "2616/2616 [==============================] - 56s 21ms/step - loss: 1.3471 - accuracy: 0.6669\n",
      "Epoch 49/120\n",
      "2616/2616 [==============================] - 57s 22ms/step - loss: 1.3232 - accuracy: 0.6731\n",
      "Epoch 50/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.3015 - accuracy: 0.6757\n",
      "Epoch 51/120\n",
      "2616/2616 [==============================] - 57s 22ms/step - loss: 1.2811 - accuracy: 0.6805\n",
      "Epoch 52/120\n",
      "2616/2616 [==============================] - 57s 22ms/step - loss: 1.2600 - accuracy: 0.6857\n",
      "Epoch 53/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.2487 - accuracy: 0.6896\n",
      "Epoch 54/120\n",
      "2616/2616 [==============================] - 56s 21ms/step - loss: 1.2275 - accuracy: 0.6948\n",
      "Epoch 55/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.2044 - accuracy: 0.6994\n",
      "Epoch 56/120\n",
      "2616/2616 [==============================] - 56s 21ms/step - loss: 1.1978 - accuracy: 0.7005\n",
      "Epoch 57/120\n",
      "2616/2616 [==============================] - 57s 22ms/step - loss: 1.1738 - accuracy: 0.7055\n",
      "Epoch 58/120\n",
      "2616/2616 [==============================] - 57s 22ms/step - loss: 1.1622 - accuracy: 0.7091\n",
      "Epoch 59/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.1423 - accuracy: 0.7141\n",
      "Epoch 60/120\n",
      "2616/2616 [==============================] - 56s 21ms/step - loss: 1.1305 - accuracy: 0.7157\n",
      "Epoch 61/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.1103 - accuracy: 0.7205\n",
      "Epoch 62/120\n",
      "2616/2616 [==============================] - 57s 22ms/step - loss: 1.0929 - accuracy: 0.7240\n",
      "Epoch 63/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.1163 - accuracy: 0.7171\n",
      "Epoch 64/120\n",
      "2616/2616 [==============================] - 57s 22ms/step - loss: 1.0601 - accuracy: 0.7329\n",
      "Epoch 65/120\n",
      "2616/2616 [==============================] - 57s 22ms/step - loss: 1.0797 - accuracy: 0.7270\n",
      "Epoch 66/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.0422 - accuracy: 0.7358\n",
      "Epoch 67/120\n",
      "2616/2616 [==============================] - 56s 21ms/step - loss: 1.0412 - accuracy: 0.7354\n",
      "Epoch 68/120\n",
      "2616/2616 [==============================] - 56s 21ms/step - loss: 1.1378 - accuracy: 0.7114\n",
      "Epoch 69/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.0187 - accuracy: 0.7418\n",
      "Epoch 70/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.0200 - accuracy: 0.7409\n",
      "Epoch 71/120\n",
      "2616/2616 [==============================] - 56s 22ms/step - loss: 1.0042 - accuracy: 0.7449\n",
      "Epoch 72/120\n",
      "2616/2616 [==============================] - 56s 21ms/step - loss: 0.9985 - accuracy: 0.7474\n",
      "Epoch 73/120\n",
      "2616/2616 [==============================] - 56s 21ms/step - loss: 0.9863 - accuracy: 0.7485\n",
      "Epoch 74/120\n",
      "2616/2616 [==============================] - 56s 21ms/step - loss: 0.9780 - accuracy: 0.7508\n",
      "Epoch 75/120\n",
      "2616/2616 [==============================] - 56s 21ms/step - loss: 0.9667 - accuracy: 0.7542\n",
      "Epoch 76/120\n",
      "2616/2616 [==============================] - 56s 21ms/step - loss: 0.9538 - accuracy: 0.7560\n",
      "Epoch 77/120\n",
      "2616/2616 [==============================] - 58s 22ms/step - loss: 0.9437 - accuracy: 0.7609\n",
      "Epoch 78/120\n",
      "2616/2616 [==============================] - 71s 27ms/step - loss: 0.9356 - accuracy: 0.7606\n",
      "Epoch 79/120\n",
      "2616/2616 [==============================] - 68s 26ms/step - loss: 0.9302 - accuracy: 0.7628\n",
      "Epoch 80/120\n",
      "2616/2616 [==============================] - 71s 27ms/step - loss: 0.9178 - accuracy: 0.7658\n",
      "Epoch 81/120\n",
      "2616/2616 [==============================] - 65s 25ms/step - loss: 0.9094 - accuracy: 0.7682\n",
      "Epoch 82/120\n",
      "2616/2616 [==============================] - 65s 25ms/step - loss: 0.9034 - accuracy: 0.7685\n",
      "Epoch 83/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 0.8912 - accuracy: 0.7723\n",
      "Epoch 84/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 0.8813 - accuracy: 0.7735\n",
      "Epoch 85/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 0.8791 - accuracy: 0.7751\n",
      "Epoch 86/120\n",
      "2616/2616 [==============================] - 67s 25ms/step - loss: 0.8750 - accuracy: 0.7749\n",
      "Epoch 87/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 0.8583 - accuracy: 0.7805\n",
      "Epoch 88/120\n",
      "2616/2616 [==============================] - 68s 26ms/step - loss: 0.8601 - accuracy: 0.7788\n",
      "Epoch 89/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 0.8426 - accuracy: 0.7843\n",
      "Epoch 90/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 0.8449 - accuracy: 0.7834\n",
      "Epoch 91/120\n",
      "2616/2616 [==============================] - 65s 25ms/step - loss: 0.8337 - accuracy: 0.7862\n",
      "Epoch 92/120\n",
      "2616/2616 [==============================] - 65s 25ms/step - loss: 0.8336 - accuracy: 0.7857\n",
      "Epoch 93/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 0.8246 - accuracy: 0.7869\n",
      "Epoch 94/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 0.8158 - accuracy: 0.7896\n",
      "Epoch 95/120\n",
      "2616/2616 [==============================] - 65s 25ms/step - loss: 0.8099 - accuracy: 0.7907\n",
      "Epoch 96/120\n",
      "2616/2616 [==============================] - 65s 25ms/step - loss: 0.8065 - accuracy: 0.7926\n",
      "Epoch 97/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 0.8083 - accuracy: 0.7924\n",
      "Epoch 98/120\n",
      "2616/2616 [==============================] - 66s 25ms/step - loss: 0.7998 - accuracy: 0.7924\n",
      "Epoch 99/120\n",
      "2616/2616 [==============================] - 67s 25ms/step - loss: 0.7918 - accuracy: 0.7961\n",
      "Epoch 100/120\n",
      "2616/2616 [==============================] - 65s 25ms/step - loss: 0.7823 - accuracy: 0.7990\n",
      "Epoch 101/120\n",
      "2616/2616 [==============================] - 73s 28ms/step - loss: 0.7853 - accuracy: 0.7985\n",
      "Epoch 102/120\n",
      "2616/2616 [==============================] - 72s 28ms/step - loss: 0.7766 - accuracy: 0.7992\n",
      "Epoch 103/120\n",
      "2616/2616 [==============================] - 74s 28ms/step - loss: 0.7708 - accuracy: 0.8016\n",
      "Epoch 104/120\n",
      "2616/2616 [==============================] - 70s 27ms/step - loss: 0.7739 - accuracy: 0.8004\n",
      "Epoch 105/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 0.7633 - accuracy: 0.8025\n",
      "Epoch 106/120\n",
      "2616/2616 [==============================] - 62s 24ms/step - loss: 0.7574 - accuracy: 0.8044\n",
      "Epoch 107/120\n",
      "2616/2616 [==============================] - 62s 24ms/step - loss: 0.7507 - accuracy: 0.8069\n",
      "Epoch 108/120\n",
      "2616/2616 [==============================] - 64s 24ms/step - loss: 0.7513 - accuracy: 0.8067\n",
      "Epoch 109/120\n",
      "2616/2616 [==============================] - 61s 23ms/step - loss: 0.7428 - accuracy: 0.8083\n",
      "Epoch 110/120\n",
      "2616/2616 [==============================] - 61s 23ms/step - loss: 0.7440 - accuracy: 0.8073\n",
      "Epoch 111/120\n",
      "2616/2616 [==============================] - 60s 23ms/step - loss: 0.7353 - accuracy: 0.8110\n",
      "Epoch 112/120\n",
      "2616/2616 [==============================] - 60s 23ms/step - loss: 0.7323 - accuracy: 0.8110\n",
      "Epoch 113/120\n",
      "2616/2616 [==============================] - 60s 23ms/step - loss: 0.7328 - accuracy: 0.8112\n",
      "Epoch 114/120\n",
      "2616/2616 [==============================] - 61s 23ms/step - loss: 0.7295 - accuracy: 0.8125\n",
      "Epoch 115/120\n",
      "2616/2616 [==============================] - 64s 24ms/step - loss: 0.7229 - accuracy: 0.8144\n",
      "Epoch 116/120\n",
      "2616/2616 [==============================] - 65s 25ms/step - loss: 0.7250 - accuracy: 0.8129\n",
      "Epoch 117/120\n",
      "2616/2616 [==============================] - 68s 26ms/step - loss: 0.7181 - accuracy: 0.8148\n",
      "Epoch 118/120\n",
      "2616/2616 [==============================] - 67s 26ms/step - loss: 0.7104 - accuracy: 0.8172\n",
      "Epoch 119/120\n",
      "2616/2616 [==============================] - 63s 24ms/step - loss: 0.7105 - accuracy: 0.8179\n",
      "Epoch 120/120\n",
      "2616/2616 [==============================] - 75s 29ms/step - loss: 0.7019 - accuracy: 0.8196\n"
     ]
    }
   ],
   "source": [
    "history_bidirectional = model_bidirectonal.fit(x_train, y_train, epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 758ms/step\n",
      "The note was undated, and without either signature or ,prediction: address\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The note was undated, and without either signature or\"\n",
    "word = predict_next_word(model=model_bidirectonal, input_text=input_text, tokenizer=tokenizer, max_sequence_len=max_sequence_len)\n",
    "print(f\"{input_text} ,prediction: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "Peculiar—that is the very word,” said ,prediction: he\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Peculiar—that is the very word,” said\"\n",
    "word = predict_next_word(model=model_bidirectonal, input_text=input_text, tokenizer=tokenizer, max_sequence_len=max_sequence_len)\n",
    "print(f\"{input_text} ,prediction: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "“Very, indeed. And what does she propose to do with the ,prediction: photograph\n"
     ]
    }
   ],
   "source": [
    "input_text = \"“Very, indeed. And what does she propose to do with the\"\n",
    "word = predict_next_word(model=model_bidirectonal, input_text=input_text, tokenizer=tokenizer, max_sequence_len=max_sequence_len)\n",
    "print(f\"{input_text} ,prediction: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_bidirectonal.save(\"next_word_predition_using_Bidirectional.h5\")\n",
    "with open('tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
